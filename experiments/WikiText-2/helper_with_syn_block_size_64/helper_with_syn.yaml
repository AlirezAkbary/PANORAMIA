base:
  log_dir: /home/aaa208/scratch/PANORAMIA/experiments/WikiText-2/helper_with_syn_block_size_64/
  project_name: PANORAMIA-Wiki-Text-2-helper_with_syn_block_size_64
dataset:
  path: wikitext
  name: wikitext-2-raw-v1
  path_to_synthetic_data: ~
  synthetic_text_column_name: text
  seed: 42
  do_shuffle: True
  pretrained_model_name_or_path: gpt2
  block_size: 64
  generator_train_percent: 44
  prompt_sampling_percent: 24
  target_model_percent: 90
  helper_model_percent: 0
  mia_num_train : 250
  mia_num_val : 250
  mia_num_test : 500
  include_synthetic: True
  audit_mode:  RMFMRNFN
  num_syn_canary: 2241
generator:
  train:
    pretrained_model_name_or_path: gpt2
    saving_dir: /home/aaa208/scratch/PANORAMIA/outputs/WikiText-2/generator/helper_with_syn/saved_model/
    run_name: generator-fine-tune-helper-with-syn
    seed: 42
    optimization:
      per_device_batch_size: 64
      epoch: 60
      learning_rate: 0.00002
      weight_decay: 0.01
      warmup_steps: 100

  generation:
    saving_dir: /home/aaa208/scratch/PANORAMIA/outputs/WikiText-2/generator/helper_with_syn/saved_synthetic_data/
    syn_file_name: syn_data.csv
    save_loss_on_target: False
    parameters:
      batch_size: 128
      prompt_sequence_length: 64
      max_length: 128
      top_k: 200
      top_p: 1
      temperature: 1
      num_return_sequences: 1
audit:
  target:
    pretrained_model_name_or_path: gpt2
    saving_dir: /home/aaa208/scratch/PANORAMIA/outputs/WikiText-2/audit_model/target_with_Syn/saved_model/epoch_60/
    seed: 42
    run_name: target_epoch_60_block_size_64_syn_in_2241
    train_with_DP: False
    embedding_type: loss_seq
    optimization: 
      learning_rate: 0.00002
      weight_decay: 0.01
      warmup_steps: 100
      batch_size: 64
      epoch: 60
      save_strategy: 'no'
      load_best_model_at_end: False
      save_total_limit: ~
  helper:
    pretrained_model_name_or_path: gpt2
    saving_dir: /home/aaa208/scratch/PANORAMIA/outputs/WikiText-2/audit_model/helper/saved_model/epoch_30/checkpoint-726/
    seed: 42
    run_name: ~ #it should be already trained with RMFN (we don't need to include syn in helper). Update the saving_dir
    embedding_type: loss_seq
    optimization:
      learning_rate: 0.00002
      weight_decay: 0.01
      warmup_steps: 100
      batch_size: 64
      epoch: 15
      save_strategy: epoch
      load_best_model_at_end: True
      save_total_limit: 1
attack:
  mia:
    net_type: mix
    distinguisher_type: GPT2Distinguisher
    run_name: RMFMRNFN
    training_args:
      seed: 0  #check
      output_dir: /scratch/aaa208/PANORAMIA/outputs/WikiText-2/attacks/
      max_steps: 500
      batch_size: 32
      warmup_steps: 500
      weight_decay: 0.01
      learning_rate: 0.00003
      reg_coef:  0
      phase1_max_steps: 500
      phase1_batch_size: 32
      phase1_learning_rate: 0.03
      phase1_reg_coef: 1
      logging_steps: 10
      save_strategy: 'no'
      evaluation_strategy: 'epoch'
      overwrite_output_dir: True
      max_fpr: 0.1   
  baseline:
    net_type: mix
    distinguisher_type: GPT2Distinguisher
    run_name: RMFMRNFN
    training_args:
      seed: 0  #check
      output_dir: /scratch/aaa208/PANORAMIA/outputs/WikiText-2/attacks/
      max_steps: 500
      batch_size: 32
      warmup_steps: 500
      weight_decay: 0.01
      learning_rate: 0.00003
      reg_coef:  0
      phase1_max_steps: 250
      phase1_batch_size: 32
      phase1_learning_rate: 0.03
      phase1_reg_coef: 1
      logging_steps: 10
      save_strategy: 'no'
      evaluation_strategy: 'epoch'
      overwrite_output_dir: True
      max_fpr: 0.1   
